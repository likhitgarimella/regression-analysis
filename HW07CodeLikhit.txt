# Likhit Garimella# Regression Analysis HW-7# libraries#install.packages("faraway")#install.packages("olsrr")#install.packages("readr")library(faraway)library(olsrr)library(readr)data <- read.csv("/Users/ravi/Downloads/ridge_data.csv",show_col_types = FALSE)head(data, 5)model <- lm(y~., data = data)summary(model)step(model, direction = "forward")step(model, direction = "forward", scope = formula(model))#a. using forward stepwise selection. Specify your chosen model.ols_step_forward_p(model, details=TRUE)# The forward selection method is used to build a predictive model by iteratively adding variables based on their p-values.# In Step 1, the model includes only variable x1, with an R-squared of 0.761 and a significant p-value.# In Step 2, variable x4 is added to the model, resulting in an improved R-squared of 0.777, but its p-value is not significant (p=0.171).# Therefore, no more variables are added, and the final model consists of variables x1 and x4, with an R-squared of 0.777.ols_step_forward_aic(model, details=TRUE)# The forward selection method is used to select variables for a predictive model based on their AIC values.# In Step 0, the initial model with no variables has an AIC of 198.2403.# In Step 1, variable x1 is added to the model, resulting in a lower AIC of 157.3416.# In Step 2, variable x4 is added to the model, further reducing the AIC to 157.2249.# Since no more variables are available, the final model consists of variables x1 and x4.# The final model has an R-squared of 0.777, indicating that 77.7% of the variance in the response variable is explained by the model.#b. using backward stepwise elimination selection. Specify your chosen model. ols_step_backward_p(model, details=TRUE)# Backward elimination is a feature selection method used in regression analysis.# It involves iteratively removing variables with high p-values until a desired threshold (typically 0.05 or 0.1) is reached.# In this example, variables x4, x1, x9, x2, and x3 were eliminated.# The final model includes variables x5, x8, and x10, which together explain about 80.4% of the variance in the dependent variable.# The model has a root mean square error (RMSE) of 2.934, indicating the average difference between predicted and actual values.ols_step_backward_aic(model, details=TRUE)# Backward stepwise elimination is a variable selection method that uses AIC values to sequentially remove variables from a model.# In this data, variables x4, x1, x9, x2, and x3 were successively eliminated based on their highest AIC values.# The final model includes variables x5, x8, and x10, resulting in an AIC of 155.426, R-squared of 0.804, and RMSE of 2.934.# This approach simplifies the model while retaining significant predictors, enhancing interpretability and potentially preventing overfitting.#c. using bi-directional stepwise selection. Specify your chosen model.ols_step_both_p(model, details=TRUE)# Bi-directional stepwise selection combines forward and backward stepwise elimination to iteratively add significant variables# (based on p-values) and remove variables with the highest AIC value.# In the given data, the method chose a final model containing only variable x1, with a significant negative coefficient (-0.047).# This model explains 76.1% of the variance in the response variable (R-squared = 0.761) and has an RMSE of 3.120, demonstrating effective data explanation and prediction.ols_step_both_aic(model, details=TRUE)# Bi-directional stepwise selection is a variable selection method that combines forward and backward stepwise elimination based on AIC values.# It aims to find the most parsimonious model by adding significant variables and removing variables with high AIC values.# The final model includes variables x1 and x4.# Variable x1 has a negative coefficient (-0.044), indicating a negative relationship with the response variable.# Variable x4 has a positive coefficient (3.077).# The chosen model achieves an R-squared of 0.777, indicating that it explains 77.7% of the variance in the response variable.# The model has an RMSE of 3.067, demonstrating its ability to predict the data effectively.#d. perform all possible regressions using variables x1, x2, x3, x4 , x5, x8model1 = lm(y ~ x1+x2+x3+x4+x5+x8,data=data)ols_step_all_possible(model1)# Metric - R square# To choose the best model among all possible regressions using the R-squared metric,# we look for the highest R-squared value as it indicates the proportion of the variance in the dependent variable that can be explained by the predictors.# We can see that the model with the highest R-squared value is Model 63, which includes all six predictors (x1, x2, x3, x4, x5, x8).# It has an R-squared value of 0.7848886.# The high R-squared value suggests that Model 63 explains approximately 78.49% of the variance in the dependent variable.# Therefore, based on the R-squared metric, Model 63 would be considered the best model among all possible regressions as it provides the highest level of explained variance.# Metric - Adjusted R square# The adjusted R-squared takes into account both the goodness of fit and the number of predictors in the model,# providing a measure of the model's explanatory power while penalizing for overfitting.# We can see that the model with the highest adjusted R-squared is the one with 6 predictors (x1, x2, x3, x4, x5, x8).# It has an adjusted R-squared of 0.7287725,# indicating that approximately 72.9% of the variability in the response variable (y) can be explained by the predictors in the model, while considering the degrees of freedom.# It provides a good balance between model fit and complexity, suggesting that these predictors collectively contribute significantly to explaining the variation in the response variable.